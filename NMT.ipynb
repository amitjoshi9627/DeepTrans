{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwcuTtAAFSrT",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJFFsJjuYIth",
        "colab_type": "code",
        "outputId": "8c402144-ab51-4b02-82c9-e7a4762ccb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-13 06:38:28--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7612057 (7.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   7.26M  3.35MB/s    in 2.2s    \n",
            "\n",
            "2019-12-13 06:38:31 (3.35 MB/s) - ‘deu-eng.zip’ saved [7612057/7612057]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5oVJZyGTE56",
        "colab_type": "code",
        "outputId": "21115ed8-3707-4e53-fe58-39e69033478e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qIcKxoPOkux",
        "colab_type": "code",
        "outputId": "c53f969a-5977-457b-c7df-ef2bc2ed7f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# !git clone https://github.com/kmsravindra/ML-AI-experiments.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML-AI-experiments'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (205/205), 14.28 MiB | 12.97 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swE-WWM5PJmo",
        "colab_type": "code",
        "outputId": "98ab1f25-4e51-41d9-8014-ffb77ee4e02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# cd /content/ML-AI-experiments/AI/Neural\\ Machine\\ Translation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ML-AI-experiments/AI/Neural Machine Translation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkPxLD_HQdo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSp5h_2HOFao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SQpZOz-h3ULY",
        "colab": {}
      },
      "source": [
        "lines = open('/content/deu.txt', encoding='utf-8').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e63de7d9-71bd-4f9b-b0b1-5089e43613a8",
        "id": "0LSGiznW-_Xk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Cu7WgJPr3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_sent = []\n",
        "fra_sent = []\n",
        "eng_chars = set()\n",
        "fra_chars = set()\n",
        "nb_samples = 20000\n",
        "\n",
        "# Process english and french sentences\n",
        "for line in range(nb_samples):\n",
        "    \n",
        "    eng_line = str(lines[line]).split('\\t')[0]\n",
        "    \n",
        "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
        "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
        "    eng_sent.append(eng_line.lower())\n",
        "    fra_sent.append(fra_line.lower())\n",
        "    \n",
        "    for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.add(ch)\n",
        "            \n",
        "    for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.add(ch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG0YAu1FPv-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fra_chars = sorted(list(fra_chars))\n",
        "eng_chars = sorted(list(eng_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77MiWHhkPzES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzeTscxHP06O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x2iQDWyP2vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss8b8fDaP42r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ-eL2d7NRu_",
        "colab_type": "code",
        "outputId": "6cf0be56-48b3-4084-e192-001f85b16035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(eng_chars),len(fra_chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 91)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVXaD29sQQ0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k,ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
        "        \n",
        "    for k,ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKghysCeQTp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder model\n",
        "\n",
        "encoder_input = Input(shape=(None,len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256,return_state = True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Qnqw_XQWJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Decoder model\n",
        "\n",
        "decoder_input = Input(shape=(None,len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
        "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
        "decoder_out = decoder_dense (decoder_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzZBVj1Qh8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kmU3PHDX_or",
        "colab_type": "code",
        "outputId": "e31cbfaa-c223-4115-9dc8-9bed68867c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "for i in range(5):\n",
        "    model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "            y=target_data,\n",
        "            batch_size=64,\n",
        "            epochs=10,\n",
        "            validation_split=0.2)\n",
        "    print(f'{i}'*20)\n",
        "    model.save('NMT.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.1017 - val_loss: 0.3992\n",
            "Epoch 2/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0999 - val_loss: 0.4026\n",
            "Epoch 3/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0983 - val_loss: 0.4045\n",
            "Epoch 4/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0966 - val_loss: 0.4088\n",
            "Epoch 5/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0947 - val_loss: 0.4139\n",
            "Epoch 6/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0931 - val_loss: 0.4134\n",
            "Epoch 7/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0913 - val_loss: 0.4190\n",
            "Epoch 8/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0901 - val_loss: 0.4207\n",
            "Epoch 9/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0885 - val_loss: 0.4250\n",
            "Epoch 10/10\n",
            "16000/16000 [==============================] - 27s 2ms/step - loss: 0.0870 - val_loss: 0.4309\n",
            "00000000000000000000\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            " 8896/16000 [===============>..............] - ETA: 11s - loss: 0.0834"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-c8ae0b2821a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             validation_split=0.2)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i}'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NMT.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4tffeK1HcIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('NMT_final.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyooGcGbQkCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tYilBBSqQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_seq(inp_seq):\n",
        "    \n",
        "    # Initial states value is coming from the encoder \n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "    \n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "    \n",
        "    while not stop_condition:\n",
        "        \n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "        \n",
        "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "        \n",
        "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "        \n",
        "        states_val = [decoder_h, decoder_c]\n",
        "        \n",
        "    return translated_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-myLdHhPSyki",
        "colab_type": "code",
        "outputId": "e941b7c0-0e7b-4061-ae42-13794e995be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "for seq_index in range(10):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: hi.\n",
            "Decoded sentence: hallo!\n",
            "\n",
            "-\n",
            "Input sentence: hi.\n",
            "Decoded sentence: hallo!\n",
            "\n",
            "-\n",
            "Input sentence: run!\n",
            "Decoded sentence: weille!\n",
            "\n",
            "-\n",
            "Input sentence: wow!\n",
            "Decoded sentence: donnerwetter!\n",
            "\n",
            "-\n",
            "Input sentence: wow!\n",
            "Decoded sentence: donnerwetter!\n",
            "\n",
            "-\n",
            "Input sentence: fire!\n",
            "Decoded sentence: feuer!\n",
            "\n",
            "-\n",
            "Input sentence: help!\n",
            "Decoded sentence: hilfollo!\n",
            "\n",
            "-\n",
            "Input sentence: help!\n",
            "Decoded sentence: hilfollo!\n",
            "\n",
            "-\n",
            "Input sentence: stop!\n",
            "Decoded sentence: stopp!\n",
            "\n",
            "-\n",
            "Input sentence: wait!\n",
            "Decoded sentence: wartet mal!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQraR5ULNUt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sent = \"What is your name?\".lower()\n",
        "max_len_test = len(test_sent)\n",
        "tokenized_test_sentences = np.zeros(shape = (nb_samples,max_len_test,len(eng_chars)), dtype='float32')\n",
        "for k,ch in enumerate(test_sent):\n",
        "    tokenized_test_sentences[0,k,eng_char_to_index_dict[ch]] = 1\n",
        "test_sent2 = \"How are you?\".lower()\n",
        "max_len_test = len(test_sent2)\n",
        "tokenized_test_sentences2 = np.zeros(shape = (nb_samples,max_len_test,len(eng_chars)), dtype='float32')\n",
        "for k,ch in enumerate(test_sent2):\n",
        "    tokenized_test_sentences2[0,k,eng_char_to_index_dict[ch]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGPVvVsIzWg",
        "colab_type": "code",
        "outputId": "d5901d58-2ac9-4a35-d3ad-d55738c1a07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(10):\n",
        "    model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
        "            y=target_data,\n",
        "            batch_size=64,\n",
        "            epochs=2,\n",
        "            validation_split=0.2)\n",
        "    print('-'*50)\n",
        "    print(f\"Iteration: {i+1}\")\n",
        "    inp_seq = tokenized_test_sentences2\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('Input sentence:', test_sent)\n",
        "    print('Decoded sentence:', translated_sent)\n",
        "    inp_seq = tokenized_test_sentences\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', test_sent)\n",
        "    print('Decoded sentence:', translated_sent)\n",
        "    model.save('NMT.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0808 - val_loss: 0.4470\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0792 - val_loss: 0.4508\n",
            "--------------------------------------------------\n",
            "Iteration: 1\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich bin den gute nachen.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist lieben wir?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0778 - val_loss: 0.4517\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0766 - val_loss: 0.4555\n",
            "--------------------------------------------------\n",
            "Iteration: 2\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich benötige einen hut.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist los eine wetter?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0755 - val_loss: 0.4594\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0742 - val_loss: 0.4626\n",
            "--------------------------------------------------\n",
            "Iteration: 3\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich bin der verlieren zu hart.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist lieben da?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0732 - val_loss: 0.4690\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0721 - val_loss: 0.4676\n",
            "--------------------------------------------------\n",
            "Iteration: 4\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich benötige sie.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist lieben wir?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 29s 2ms/step - loss: 0.0710 - val_loss: 0.4757\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0699 - val_loss: 0.4757\n",
            "--------------------------------------------------\n",
            "Iteration: 5\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich bin dir gegingen.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist mit den fegner?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0690 - val_loss: 0.4808\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0681 - val_loss: 0.4791\n",
            "--------------------------------------------------\n",
            "Iteration: 6\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich bin dir gegangen.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist los eutm lügerig?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0670 - val_loss: 0.4823\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 28s 2ms/step - loss: 0.0661 - val_loss: 0.4881\n",
            "--------------------------------------------------\n",
            "Iteration: 7\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: ich benötige sie.\n",
            "\n",
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist los eine sot?\n",
            "\n",
            "Train on 16000 samples, validate on 4000 samples\n",
            "Epoch 1/2\n",
            "14208/16000 [=========================>....] - ETA: 2s - loss: 0.0647"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTWuf0FhSwHT",
        "colab_type": "code",
        "outputId": "2ab021f8-93b3-45fa-a123-fff6f02df841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "inp_seq = tokenized_test_sentences\n",
        "translated_sent = decode_seq(inp_seq)\n",
        "print('-')\n",
        "print('Input sentence:', test_sent)\n",
        "print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: what is your name?\n",
            "Decoded sentence: was ist lieben da?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS99KlqaHyed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}